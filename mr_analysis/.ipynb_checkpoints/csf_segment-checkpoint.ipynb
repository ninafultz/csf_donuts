{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4884d72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distutils Version classes are deprecated. Use packaging.version instead.\n",
      "Setuptools is replacing distutils.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Path to your user-installed Python packages (adjust this path accordingly)\n",
    "user_site_packages = os.path.expanduser('~/.local/lib/python3.10/site-packages')\n",
    "\n",
    "# Add the directory to sys.path temporarily\n",
    "if user_site_packages not in sys.path:\n",
    "    sys.path.insert(0, user_site_packages)\n",
    "\n",
    "# Importing nilearn and itk after sys.path modification\n",
    "import nilearn\n",
    "from nilearn import image, plotting\n",
    "from nilearn.input_data import NiftiMasker\n",
    "\n",
    "import itk\n",
    "from distutils.version import StrictVersion as VS\n",
    "\n",
    "# Optionally remove from sys.path after use\n",
    "if user_site_packages in sys.path:\n",
    "    sys.path.remove(user_site_packages)\n",
    "\n",
    "# Checking ITK version requirement\n",
    "if VS(itk.Version.GetITKVersion()) < VS(\"5.0.0\"):\n",
    "    print(\"ITK 5.0.0 or newer is required.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Proceed with the rest of your imports and configuration\n",
    "import pip\n",
    "from time import perf_counter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from sklearn import cluster\n",
    "\n",
    "from skimage import exposure\n",
    "from skimage.draw import line_nd, line_aa\n",
    "from skimage.transform import rescale, resize\n",
    "from skimage.filters import gaussian\n",
    "from skimage import morphology\n",
    "from skimage.filters import threshold_otsu, threshold_local\n",
    "from skimage.morphology import binary_dilation\n",
    "\n",
    "from scipy import ndimage as ndi\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "from numba import njit, jit, prange\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = [15, 15]\n",
    "mpl.rcParams.update({'font.size': 22})\n",
    "np.set_printoptions(formatter={'float_kind':'{:0.4f}'.format})\n",
    "\n",
    "\n",
    "import os  # Import the os module at the beginning of your script\n",
    "\n",
    "\n",
    "###############################################\n",
    "### Set these:\n",
    "###############################################\n",
    "root = \"/exports/gorter-hpc/users/ninafultz/highres_pvs/20240307_PVS3\"  # Root folder of your project\n",
    "out_folder = os.path.join(root, \"segment/\")\n",
    "biasfield_folder = os.path.join(root, \"biasfield/\")\n",
    "\n",
    "# Change current working directory to biasfield_folder\n",
    "os.chdir(biasfield_folder)\n",
    "\n",
    "input_img_url = os.path.join(biasfield_folder, \"merged_last_three_DelRec_-_pvs_70slices_ME.nii.gz\")\n",
    "input_mask_url = os.path.join(biasfield_folder, \"combined_mask.nii.gz\")\n",
    "output_prefix = os.path.join(out_folder, \"qsm_vasc_\")\n",
    "out_hessian = output_prefix + \"_hessian.nii.gz\"\n",
    "\n",
    "in_image = itk.imread(input_img_url, itk.F)\n",
    "\n",
    "if not os.path.exists(out_folder):\n",
    "    os.makedirs(out_folder) \n",
    "\n",
    "try:\n",
    "    # Attempt to read the image using itk.imread\n",
    "    in_image = itk.imread(input_img_url, itk.F)\n",
    "\n",
    "    # Get voxel size and size of the image\n",
    "    voxel_size = in_image.GetSpacing()\n",
    "    size = in_image.GetLargestPossibleRegion().GetSize()\n",
    "\n",
    "    # Example: Print out voxel size and image size\n",
    "    print(f\"Voxel Size: {voxel_size}\")\n",
    "    print(f\"Image Size: {size}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error reading image: {e}\")\n",
    "    \n",
    "    \n",
    "in_image   = itk.imread(input_img_url, itk.F)               # don't touch this\n",
    "voxel_size = in_image.GetSpacing()                          # don't touch this\n",
    "size       = in_image.GetLargestPossibleRegion().GetSize()  # don't touch this\n",
    "\n",
    "\n",
    "brightvessels = False                  # Bright=ToF, Dark=SWI,T2star\n",
    "sigma_minimum = np.mean(voxel_size)    # min voxel size (or mean), can be a little higher\n",
    "sigma_maximum = sigma_minimum*1.95     # a bit less than twice\n",
    "number_of_sigma_steps=15               #The more the better, between 10 and 15 is good\n",
    "fact_version = True                    #Factory analysis (new) vs Max pooling (classic)\n",
    "\n",
    "alpha=0.8                     #In theory, no need to adjust this\n",
    "beta=1.0                      #In theory, no need to adjust this\n",
    "gamma=100                     #50 #Play with this to ajust for the noise/blur in the vascular segmentation\n",
    "scaleoutput = True            #Do. Not. Touch. That.\n",
    "\n",
    "otsu_offset=0.3              #Between -1.0 and 1.0 -> higher = more vessels when thresholded\n",
    "    \n",
    "MIP_half_thickness=8           # Half-thickness of the MIP view\n",
    "window_half_size=50            # 30 = 60x60 zoomed window\n",
    "mip_dir=\"ax\" #\"sag\"  #\"cor\"    # choice between \"sag\" \"cor\" or other (default)\n",
    "\n",
    "# Center of the window\n",
    "ix, iy, iz  = np.int(size[0]/1.6), np.int(size[1]/1.5), np.int(size[2]/2.)\n",
    "\n",
    "\n",
    "# Load images and print them\n",
    "mask_img   = image.load_img(input_mask_url)\n",
    "mask       = mask_img.get_data()\n",
    "input_img  = image.load_img(input_img_url)\n",
    "voxel_size = mask_img.header.get_zooms()[0:2]\n",
    "\n",
    "print(\"Voxel size is: \", voxel_size)\n",
    "print(\"Image dim is: \", mask_img.shape)\n",
    "#print(\"Image dim is: \", size)\n",
    "\n",
    "if mip_dir == 'sag':\n",
    "    sx, sy, sz  = MIP_half_thickness, window_half_size, window_half_size\n",
    "    mip_axis = 0\n",
    "\n",
    "    dx, ux = np.int(ix-sx), np.int(ix+sx)\n",
    "    dy, uy = np.int(iy-sy), np.int(iy+sy)  \n",
    "    dz, uz = np.int(iz-sz), np.int(iz+sz) \n",
    "\n",
    "    mip_cube = tuple([ slice(dx,ux,1), slice(dy,uy,1),   slice(dz,uz,1)    ])\n",
    "    mip_view = tuple([ slice(dx,ux,1), slice(None,None), slice(None,None)  ])\n",
    "    sng_cube = tuple([ ix,             slice(dy,uy,1),   slice(dz,uz,1)    ])\n",
    "    sng_view = tuple([ ix,             slice(None,None), slice(None,None)  ])\n",
    "elif mip_dir == 'cor':\n",
    "    sx, sy, sz  = window_half_size, MIP_half_thickness, window_half_size\n",
    "    mip_axis = 1\n",
    "\n",
    "    dx, ux = np.int(ix-sx), np.int(ix+sx)\n",
    "    dy, uy = np.int(iy-sy), np.int(iy+sy)  \n",
    "    dz, uz = np.int(iz-sz), np.int(iz+sz) \n",
    "\n",
    "    mip_cube = tuple([ slice(dx,ux,1),   slice(dy,uy,1), slice(dz,uz,1)    ])\n",
    "    mip_view = tuple([ slice(None,None), slice(dy,uy,1), slice(None,None)  ])\n",
    "    sng_cube = tuple([ slice(dx,ux,1),   iy,             slice(dz,uz,1)    ])\n",
    "    sng_view = tuple([ slice(None,None), iy,             slice(None,None)  ])\n",
    "\n",
    "else :\n",
    "    sx, sy, sz  = window_half_size, window_half_size, MIP_half_thickness\n",
    "    mip_axis = 2\n",
    "\n",
    "    dx, ux = np.int(ix-sx), np.int(ix+sx)\n",
    "    dy, uy = np.int(iy-sy), np.int(iy+sy)  \n",
    "    dz, uz = np.int(iz-sz), np.int(iz+sz)  \n",
    "\n",
    "    mip_cube = tuple([ slice(dx,ux,1),    slice(dy,uy,1),    slice(dz,uz,1) ])\n",
    "    mip_view = tuple([ slice(None, None), slice(None, None), slice(dz,uz,1) ])\n",
    "    sng_cube = tuple([ slice(dx,ux,1),    slice(dy,uy,1),    iz             ])\n",
    "    sng_view = tuple([ slice(None, None), slice(None, None), iz             ])\n",
    "    \n",
    "#Set-up window variables\n",
    "print(\"Center: \",      ix, iy, iz)\n",
    "print(\"Zoom window: \", sx, sy, sz)\n",
    "print(\"Low limit: \",   dx, dy, dz)\n",
    "print(\"High limit: \",  ux, uy, uz)\n",
    "print(\"Size of single will be: \",        mask[sng_view].T.shape)\n",
    "print(\"Size of zoomed single will be: \", mask[sng_cube].T.shape)\n",
    "print(\"Size of MIP will be: \",           np.min(mask[mip_view], axis=mip_axis).shape)\n",
    "print(\"Size of zoomed MIP will be: \",    np.min(mask[mip_cube], axis=mip_axis).shape)\n",
    "   \n",
    "#Plot the loaded images\n",
    "fig, axes = plt.subplots(1, 3, figsize=(28, 20))\n",
    "fig.tight_layout()\n",
    "ax = axes.flatten()\n",
    "ax[0].imshow(input_img.get_data()[sng_view].T, origin='lower', cmap=plt.cm.gray)\n",
    "ax[1].imshow(np.min(input_img.get_data()[mip_view], axis=mip_axis).T, origin='lower', cmap=plt.cm.gray, alpha=1.0)\n",
    "ax[2].imshow(np.min(input_img.get_data()[mip_cube], axis=mip_axis).T, origin='lower', cmap=plt.cm.gray, alpha=1.0)\n",
    "plt.title(\"Data loaded\")\n",
    "plt.savefig(\"Loaded_data_fig.png\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def FA_combine_scales(img_4D, mask, sigma_min, sigma_max):\n",
    "    masker = NiftiMasker()\n",
    "    data_masked = masker.fit_transform(img_4D)\n",
    "    print(np.max(data_masked))\n",
    "    assert np.max(data_masked) > 0, \"Mask and data are not in same space/orient\"\n",
    "\n",
    "    \n",
    "    p0, p99 = np.percentile(data_masked, (0, 99.8)) \n",
    "    data_masked = exposure.rescale_intensity(data_masked, in_range=(p0,p99))\n",
    "\n",
    "    #print(data_masked.shape)\n",
    "\n",
    "    print(np.max(data_masked))\n",
    "\n",
    "    noise_variance_init=np.geomspace(sigma_min, sigma_max, data_masked.shape[0]) - sigma_min #[0:16]\n",
    "    print(\"Noise variance: \", noise_variance_init)\n",
    "\n",
    "    method = FactorAnalysis(n_components=1, \n",
    "                            noise_variance_init=noise_variance_init,\n",
    "                            #noise_variance_init=np.geomspace(sigma_minimum, sigma_maximum, number_of_sigma_steps),\n",
    "                            tol=0.005, max_iter=200)\n",
    "    method.fit(data_masked.T)\n",
    "    X_reduced = method.transform(data_masked.T)\n",
    "    X_reduced[X_reduced<0] = 0\n",
    "    #print(X_reduced.shape)\n",
    "\n",
    "    #print(X_reduced.noise_variance_array_)\n",
    "    fact_img = masker.inverse_transform(X_reduced.T)\n",
    "    fact_img = image.index_img(fact_img,0)\n",
    "    \n",
    "    factimg=nilearn.image.math_img(\"np.where(x<0, 0, x)\", x=fact_img)\n",
    "    \n",
    "    p0, p99 = np.percentile(factimg.get_data(), (0, 99.8)) \n",
    "    factdata = exposure.rescale_intensity(factimg.get_data(), in_range=(p0,p99))\n",
    "\n",
    "    return factdata\n",
    "\n",
    "def skeletonize_mask(i_thresh_img, i_voxel_size=0.6, i_factor=2.0):\n",
    "    thresh_data = i_thresh_img.get_data()\n",
    "    dil_thresh = binary_dilation(thresh_data)\n",
    "    skel = morphology.skeletonize_3d(dil_thresh)\n",
    "    #skel = image.load_img(outfold + output_prefix + \"_VED_skel.nii.gz\").get_data()\n",
    "    skel[skel > 0] = 1.0\n",
    "\n",
    "    prox_sampling = int(i_voxel_size/i_factor)\n",
    "\n",
    "    #up_thresh = rescale(thresh_data, i_factor)\n",
    "    #up_distance = ndi.distance_transform_edt(up_thresh, sampling=prox_sampling)\n",
    "    #distance = resize(up_distance, skel.shape) * thresh_data\n",
    "    distance = ndi.distance_transform_edt(thresh_data, sampling=i_voxel_size)\n",
    "    centerdia = skel * distance\n",
    "\n",
    "    #up_i_thresh = 1.0 - up_thresh\n",
    "    up_i_thresh = 1.0 - thresh_data\n",
    "    #up_i_distance = ndi.distance_transform_edt(up_i_thresh, sampling=prox_sampling) #get from header instead\n",
    "    up_i_distance = ndi.distance_transform_edt(up_i_thresh, sampling=i_voxel_size) #get from header instead\n",
    "    inverted_distance = up_i_distance  * up_i_thresh\n",
    "\n",
    "    return skel, centerdia, distance, inverted_distance\n",
    "\n",
    "def otsu_3D(i_img, offset=0.2):\n",
    "\n",
    "    p0, p99 = np.percentile(i_img.get_data(), (0, 99.8)) \n",
    "    datacopy = exposure.rescale_intensity(i_img.get_data(), in_range=(p0,p99))\n",
    "\n",
    "    #datacopy = fact_img.get_data()\n",
    "    threshold_x = np.zeros_like(datacopy)\n",
    "    threshold_y = np.zeros_like(datacopy)\n",
    "    threshold_z = np.zeros_like(datacopy)\n",
    "\n",
    "    offset = 0.2\n",
    "    #threshold_local\n",
    "    for x in range(datacopy.shape[0]):\n",
    "        if np.max(datacopy[x, :, :]) == 0.0:\n",
    "            val = 1.0\n",
    "        else:\n",
    "            img_adapteq = exposure.equalize_adapthist(datacopy[x,:,:], clip_limit=0.01)\n",
    "            val = threshold_otsu(img_adapteq, nbins=400 )\n",
    "            val = max(0.0, val-offset)\n",
    "            threshold_x[x, :, :] = val\n",
    "\n",
    "    for y in range(datacopy.shape[1]):\n",
    "        if np.max(datacopy[:, y, :]) == 0.0:\n",
    "            val = 1.0\n",
    "        else:\n",
    "            img_adapteq = exposure.equalize_adapthist(datacopy[:,y,:], clip_limit=0.01)\n",
    "            val = threshold_otsu(img_adapteq, nbins=400 )\n",
    "            val = max(0.0, val-offset)\n",
    "            threshold_y[:, y, :] = val\n",
    "\n",
    "    for z in range(datacopy.shape[2]):\n",
    "        if np.max(datacopy[:, :, z]) == 0.0:\n",
    "            val = 1.0\n",
    "        else:\n",
    "            img_adapteq = exposure.equalize_adapthist(datacopy[:,:,z], clip_limit=0.01)\n",
    "            val = threshold_otsu(img_adapteq, nbins=400 )\n",
    "            val = max(0.0, val-offset)\n",
    "            threshold_z[:, :, z] = val\n",
    "\n",
    "    threshold = np.stack((threshold_x, threshold_y, threshold_z), axis=3)\n",
    "    threshold_min = np.min(threshold, axis=3)\n",
    "\n",
    "    return datacopy > threshold_min\n",
    "\n",
    "print(\"+-+-+- DOING MULTI-SCALE METHOD\")\n",
    "\n",
    "ImageType = type(in_image)\n",
    "Dimension = in_image.GetImageDimension()  #would be 3\n",
    "\n",
    "HessianPixelType = itk.SymmetricSecondRankTensor[itk.D, Dimension]\n",
    "HessianImageType = itk.Image[HessianPixelType, Dimension]\n",
    "print(HessianImageType)\n",
    "\n",
    "objectness_filter = itk.HessianToObjectnessMeasureImageFilter[HessianImageType, ImageType].New()\n",
    "objectness_filter.SetBrightObject(brightvessels)\n",
    "objectness_filter.SetScaleObjectnessMeasure(scaleoutput)  #WAS FALSE per default\n",
    "objectness_filter.SetAlpha(alpha) # 0.5 default\n",
    "objectness_filter.SetBeta(beta)   # should be 1.0\n",
    "objectness_filter.SetGamma(gamma) # was 5.0 in example, 300 at high gamma.. higher = less noise\n",
    "\n",
    "\n",
    "outfold = output_prefix + '_scales/'\n",
    "if not os.path.exists(outfold):\n",
    "    os.makedirs(outfold)\n",
    "\n",
    "\n",
    "print(\"sigma_minimum is: \" + str(sigma_minimum))\n",
    "print(\"sigma_maximum is: \" + str(sigma_maximum))\n",
    "for step, sigma in enumerate(np.geomspace(sigma_minimum, sigma_maximum, number_of_sigma_steps)):\n",
    "\n",
    "    scale_str = str(int(sigma*100000)).zfill(8)\n",
    "    \n",
    "    out_scale_prefix=outfold + output_prefix + \"_\" + scale_str\n",
    "\n",
    "    if not os.path.exists(out_scale_prefix + \"_VED.nii.gz\"):\n",
    "        print(\" +-+- DOING sigma scale \" + str(sigma) + \": \" + str(step+1) + \" of \" + str(number_of_sigma_steps))\n",
    "        multi_scale_filter = itk.MultiScaleHessianBasedMeasureImageFilter[ImageType, HessianImageType, ImageType].New()\n",
    "        multi_scale_filter.SetInput(in_image)\n",
    "        multi_scale_filter.SetHessianToMeasureFilter(objectness_filter)\n",
    "        multi_scale_filter.SetSigmaStepMethodToLogarithmic()\n",
    "        multi_scale_filter.SetSigmaMinimum(sigma)\n",
    "        multi_scale_filter.SetSigmaMaximum(sigma)\n",
    "        multi_scale_filter.SetNumberOfSigmaSteps(1)\n",
    "        multi_scale_filter.SetGenerateHessianOutput(True)\n",
    "        multi_scale_filter.Update()\n",
    "\n",
    "        print(\"  +- ..write VED of sigma \" + str(sigma) + \": \" + str(step+1) + \" of \" + str(number_of_sigma_steps))\n",
    "        Hessian_output=multi_scale_filter.GetHessianOutput()\n",
    "        VED_output=multi_scale_filter.GetOutput()\n",
    "        size=Hessian_output.GetBufferedRegion().GetSize()\n",
    "\n",
    "        hessian_view = itk.GetArrayViewFromImage(Hessian_output)\n",
    "        hessian_array = np.transpose(hessian_view, (2,1,0,3))\n",
    "        ved_view = itk.GetArrayViewFromImage(VED_output)\n",
    "        ved_array = np.transpose(ved_view, (2,1,0))\n",
    "        ved_array = ved_array*mask\n",
    "\n",
    "        #print(np.min(ved_array))\n",
    "        #print(np.max(ved_array))\n",
    "        p0, p99 = np.percentile(ved_array, (0, 99.8)) \n",
    "        ved_array = exposure.rescale_intensity(ved_array, in_range=(p0,p99))\n",
    "        #print(np.min(ved_array))\n",
    "        #print(np.max(ved_array))\n",
    "    \n",
    "\n",
    "        ved_img = nilearn.image.new_img_like(input_img, ved_array, copy_header=True)\n",
    "        ved_img.to_filename(out_scale_prefix + \"_VED.nii.gz\")\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(28, 14))\n",
    "        fig.tight_layout()\n",
    "        ax = axes.flatten()\n",
    "        ax[0].imshow(np.min(input_img.get_data()[mip_view], axis=mip_axis).T, origin='lower')\n",
    "        ax[1].imshow(np.min(input_img.get_data()[mip_view], axis=mip_axis).T, origin='lower', cmap=plt.cm.gray, alpha=1.0)\n",
    "        ax[1].imshow(np.max(     ved_array[mip_view], axis=mip_axis).T, origin='lower', cmap='viridis', alpha=0.7)\n",
    "        ax[2].imshow(np.max(     ved_array[mip_view], axis=mip_axis).T, origin='lower')\n",
    "        plt.title(\"VED for: \" + str(sigma) + \": \" + str(step+1) + \" of \" + str(number_of_sigma_steps))\n",
    "        plt.savefig(out_scale_prefix + '_fig.png', bbox_inches='tight')\n",
    "        plt.show()       \n",
    "        \n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=(28, 14))\n",
    "        fig.tight_layout()\n",
    "        ax = axes.flatten()\n",
    "        ax[0].imshow(np.min(input_img.get_data()[mip_cube], axis=mip_axis).T, origin='lower')\n",
    "        ax[1].imshow(np.min(input_img.get_data()[mip_cube], axis=mip_axis).T, origin='lower', cmap=plt.cm.gray, alpha=1.0)\n",
    "        ax[1].imshow(np.max(     ved_array[mip_cube], axis=mip_axis).T, origin='lower', cmap='viridis', alpha=0.7)\n",
    "        ax[2].imshow(np.max(     ved_array[mip_cube], axis=mip_axis).T, origin='lower')\n",
    "        plt.title(\"VED for: \" + str(sigma) + \": \" + str(step+1) + \" of \" + str(number_of_sigma_steps))\n",
    "        plt.savefig(out_scale_prefix + '_ZOOM.png', bbox_inches='tight')\n",
    "        plt.show()\n",
    "        #plotting.plot_epi(ved_img, input_img, crop=True, filename=out_scale_prefix + \"_VED.png\", dpi=150)\n",
    "\n",
    "        \n",
    "\n",
    "out_classic_prefix=output_prefix + \"_CLASSIC\"\n",
    "\n",
    "if not os.path.exists(out_classic_prefix + \"_VED.nii.gz\"):\n",
    "   \n",
    "    print(\"+-+-+- DOING CLASSIC METHOD\")\n",
    "    multi_scale_filter = itk.MultiScaleHessianBasedMeasureImageFilter[ImageType, HessianImageType, ImageType].New()\n",
    "    multi_scale_filter.SetInput(in_image)\n",
    "    multi_scale_filter.SetHessianToMeasureFilter(objectness_filter)\n",
    "    multi_scale_filter.SetSigmaStepMethodToLogarithmic()\n",
    "    multi_scale_filter.SetSigmaMinimum(sigma_minimum)\n",
    "    multi_scale_filter.SetSigmaMaximum(sigma_maximum)\n",
    "    multi_scale_filter.SetNumberOfSigmaSteps(number_of_sigma_steps)\n",
    "    multi_scale_filter.SetGenerateHessianOutput(True)\n",
    "    multi_scale_filter.Update()\n",
    "    print(\"+-+-+- DONE CLASSIC\")\n",
    "\n",
    "    print(\"+-+- Check output size\")\n",
    "    Hessian_output=multi_scale_filter.GetHessianOutput()\n",
    "    VED_output=multi_scale_filter.GetOutput()\n",
    "    size=Hessian_output.GetBufferedRegion().GetSize()\n",
    "    print(\"%d,%d,%d\"%(size[0],size[1],size[2]))\n",
    "\n",
    "\n",
    "    print(\"+-+- Making new image\")\n",
    "    hessian_view = itk.GetArrayViewFromImage(Hessian_output)\n",
    "    hessian_array = np.transpose(hessian_view, (2,1,0,3))\n",
    "    ved_view = itk.GetArrayViewFromImage(VED_output)\n",
    "    ved_array = np.transpose(ved_view, (2,1,0))\n",
    "    print(hessian_array.shape)\n",
    "\n",
    "    ved_array = np.transpose(ved_view, (2,1,0))\n",
    "    ved_array = ved_array*mask\n",
    "\n",
    "    print(np.min(ved_array))\n",
    "    print(np.max(ved_array))\n",
    "    p0, p99 = np.percentile(ved_array, (0, 99.8)) \n",
    "    ved_array = exposure.rescale_intensity(ved_array, in_range=(p0,p99))\n",
    "    print(np.min(ved_array))\n",
    "    print(np.max(ved_array))\n",
    "\n",
    "    ved_img = nilearn.image.new_img_like(input_img, ved_array, copy_header=True)\n",
    "    ved_img.to_filename(out_classic_prefix + \"_VED.nii.gz\")\n",
    "\n",
    "    ####\n",
    "    # Plot: MIP around shape[1]/2\n",
    "    ###\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(28, 14))\n",
    "    fig.tight_layout()\n",
    "    ax = axes.flatten()\n",
    "    ax[0].imshow(np.min(input_img.get_data()[mip_view], axis=mip_axis).T, origin='lower')\n",
    "    ax[1].imshow(np.min(input_img.get_data()[mip_view], axis=mip_axis).T, origin='lower', cmap=plt.cm.gray, alpha=1.0)\n",
    "    ax[1].imshow(np.max(     ved_array[mip_view], axis=mip_axis).T, origin='lower', cmap='viridis', alpha=0.7)\n",
    "    ax[2].imshow(np.max(     ved_array[mip_view], axis=mip_axis).T, origin='lower')\n",
    "    plt.title(\"VED for: \" + str(sigma) + \": \" + str(step+1) + \" of \" + str(number_of_sigma_steps))\n",
    "    plt.savefig(out_classic_prefix + '_fig.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "            \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(28, 14))\n",
    "    fig.tight_layout()\n",
    "    ax = axes.flatten()\n",
    "    ax[0].imshow(np.min(input_img.get_data()[mip_cube], axis=mip_axis).T, origin='lower')\n",
    "    ax[1].imshow(np.min(input_img.get_data()[mip_cube], axis=mip_axis).T, origin='lower', cmap=plt.cm.gray, alpha=1.0)\n",
    "    ax[1].imshow(np.max(     ved_array[mip_cube], axis=mip_axis).T, origin='lower', cmap='viridis', alpha=0.7)\n",
    "    ax[2].imshow(np.max(     ved_array[mip_cube], axis=mip_axis).T, origin='lower')\n",
    "    plt.title(\"VED for: \" + str(sigma) + \": \" + str(step+1) + \" of \" + str(number_of_sigma_steps))\n",
    "    plt.savefig(out_classic_prefix + '_ZOOM.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    hessian_img = nilearn.image.new_img_like(input_img, hessian_array, copy_header=True)\n",
    "    hessian_img.to_filename(out_classic_prefix + \"_best_hessian.nii.gz\")\n",
    "\n",
    "    ved_img = nilearn.image.new_img_like(input_img, ved_array, copy_header=True)\n",
    "    ved_img.to_filename(out_classic_prefix + \"_VED.nii.gz\")\n",
    "\n",
    "else:\n",
    "    ved_img =       nilearn.image.load_img(out_classic_prefix + \"_VED.nii.gz\")\n",
    "    hessian_img =   nilearn.image.load_img(out_classic_prefix + \"_best_hessian.nii.gz\")\n",
    "    ved_array =     ved_img.get_data()\n",
    "    hessian_array = hessian_img.get_data()\n",
    "    size = ved_array.shape\n",
    "\n",
    "print(\"+-+-+- Load VED images\")\n",
    "VED_images=nilearn.image.load_img(outfold + output_prefix + \"*_VED.nii.gz\", wildcards=True)\n",
    "print(\"Shape is: \", VED_images.shape)\n",
    "\n",
    "if not os.path.exists(outfold + output_prefix + \"_VED_max.nii.gz\"):\n",
    "    print(\"+-+- Compute max image\")\n",
    "    max_VED_images=nilearn.image.math_img(\"np.max(a, axis=3)\", a=VED_images)\n",
    "    max_VED_images.to_filename(outfold + output_prefix + \"_VED_max.nii.gz\")\n",
    "else:\n",
    "    max_VED_images = image.load_img(outfold + output_prefix + \"_VED_max.nii.gz\")\n",
    "\n",
    "if not os.path.exists(outfold + output_prefix + \"_VED_FactorAnalysis.nii.gz\"):\n",
    "    print(\"+-+- Compute factory analysis\")\n",
    "    fact_data = FA_combine_scales(VED_images, mask_img, sigma_minimum, sigma_maximum)\n",
    "    fact_img = image.new_img_like(input_img, fact_data)\n",
    "    fact_img.to_filename(outfold + output_prefix + \"_VED_FactorAnalysis.nii.gz\") \n",
    "else:\n",
    "    fact_img=image.load_img(outfold + output_prefix + \"_VED_FactorAnalysis.nii.gz\")\n",
    "    \n",
    "if fact_version:\n",
    "    ved_img = fact_img\n",
    "else:\n",
    "    ved_img = max_VED_images\n",
    "\n",
    "if not os.path.exists(outfold + output_prefix + \"_VED_otsu_thresh.nii.gz\"):\n",
    "    print(\"+-+- Compute OTSU 3D analysis\")\n",
    "    thr_data = otsu_3D(ved_img, offset=otsu_offset)\n",
    "    otsu_tresh_img = image.new_img_like(ved_img, thr_data)\n",
    "    otsu_tresh_img.to_filename(outfold + output_prefix + \"_VED_otsu_thresh.nii.gz\")\n",
    "else:\n",
    "    otsu_tresh_img=image.load_img(outfold + output_prefix + \"_VED_otsu_thresh.nii.gz\")\n",
    "\n",
    "    \n",
    "thresh_img = otsu_tresh_img\n",
    "ved_data = fact_img.get_data()\n",
    "thresh_data = otsu_tresh_img.get_data()\n",
    "\n",
    "#Print results\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(28, 12))\n",
    "fig.tight_layout()\n",
    "ax = axes.flatten()\n",
    "ax[0].imshow(input_img.get_data()[sng_view].T, origin='lower')\n",
    "ax[1].imshow(ved_img.get_data()[sng_view].T, origin='lower', cmap='viridis')\n",
    "ax[2].imshow(otsu_tresh_img.get_data()[sng_view].T, origin='lower', cmap='viridis')\n",
    "plt.savefig(outfold + output_prefix + '_VED_processed.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(28, 12))\n",
    "fig.tight_layout()\n",
    "ax = axes.flatten()\n",
    "ax[0].imshow(np.min(input_img.get_data()[mip_view], axis=mip_axis).T, origin='lower')\n",
    "ax[1].imshow(np.max(ved_img.get_data()[mip_view], axis=mip_axis).T, origin='lower', cmap='viridis')\n",
    "ax[2].imshow(np.max(otsu_tresh_img.get_data()[mip_view], axis=mip_axis).T, origin='lower', cmap='viridis')\n",
    "plt.savefig(outfold + output_prefix + '_VED_processed_MIP.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(28, 12))\n",
    "fig.tight_layout()\n",
    "ax = axes.flatten()\n",
    "ax[0].imshow(np.min(input_img.get_data()[mip_cube], axis=mip_axis).T, origin='lower')\n",
    "ax[1].imshow(np.max(ved_img.get_data()[mip_cube], axis=mip_axis).T, origin='lower', cmap='viridis')\n",
    "ax[2].imshow(np.max(otsu_tresh_img.get_data()[mip_cube], axis=mip_axis).T, origin='lower', cmap='viridis')\n",
    "plt.savefig(outfold + output_prefix + '_VED_processed_MIP_ZOOM.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3e944f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
