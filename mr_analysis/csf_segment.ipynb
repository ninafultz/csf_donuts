{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f513a35c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "`np.sctypes` was removed in the NumPy 2.0 release. Access dtypes explicitly instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Importing nilearn and itk after sys.path modification\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnilearn\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnilearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m image, plotting\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnilearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minput_data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NiftiMasker\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitk\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nilearn/image/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Mathematical operations working on Niimg-like objects.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mLike, for example, a (3+)D block of data, and an affine.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     binarize_img,\n\u001b[1;32m      8\u001b[0m     clean_img,\n\u001b[1;32m      9\u001b[0m     concat_imgs,\n\u001b[1;32m     10\u001b[0m     copy_img,\n\u001b[1;32m     11\u001b[0m     crop_img,\n\u001b[1;32m     12\u001b[0m     get_data,\n\u001b[1;32m     13\u001b[0m     high_variance_confounds,\n\u001b[1;32m     14\u001b[0m     index_img,\n\u001b[1;32m     15\u001b[0m     iter_img,\n\u001b[1;32m     16\u001b[0m     largest_connected_component_img,\n\u001b[1;32m     17\u001b[0m     load_img,\n\u001b[1;32m     18\u001b[0m     math_img,\n\u001b[1;32m     19\u001b[0m     mean_img,\n\u001b[1;32m     20\u001b[0m     new_img_like,\n\u001b[1;32m     21\u001b[0m     smooth_img,\n\u001b[1;32m     22\u001b[0m     swap_img_hemispheres,\n\u001b[1;32m     23\u001b[0m     threshold_img,\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     26\u001b[0m     coord_transform,\n\u001b[1;32m     27\u001b[0m     reorder_img,\n\u001b[1;32m     28\u001b[0m     resample_img,\n\u001b[1;32m     29\u001b[0m     resample_to_img,\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     32\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinarize_img\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_img\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mswap_img_hemispheres\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     54\u001b[0m ]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nilearn/image/image.py:14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnibabel\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Memory, Parallel, delayed\n",
      "File \u001b[0;32m/share/software/neuroImaging/fsl/6.0.6/lib/python3.10/site-packages/nibabel/__init__.py:40\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124mQuickstart\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124m==========\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124mFor more detailed information see the :ref:`manual`.\u001b[39m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# module imports\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m analyze \u001b[38;5;28;01mas\u001b[39;00m ana\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spm99analyze \u001b[38;5;28;01mas\u001b[39;00m spm99\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spm2analyze \u001b[38;5;28;01mas\u001b[39;00m spm2\n",
      "File \u001b[0;32m/share/software/neuroImaging/fsl/6.0.6/lib/python3.10/site-packages/nibabel/analyze.py:87\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\" Read / write access to the basic Mayo Analyze format\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m===========================\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03mconstrain the affine.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvolumeutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (native_code, swapped_code, make_dt_codes,\n\u001b[1;32m     88\u001b[0m                           shape_zoom_affine, array_from_file, seek_tell,\n\u001b[1;32m     89\u001b[0m                           apply_read_scaling)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marraywriters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (make_array_writer, get_slope_inter, WriterError,\n\u001b[1;32m     91\u001b[0m                            ArrayWriter)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrapstruct\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabeledWrapStruct\n",
      "File \u001b[0;32m/share/software/neuroImaging/fsl/6.0.6/lib/python3.10/site-packages/nibabel/volumeutils.py:21\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m reduce\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcasting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m shared_range, OK_FLOATS\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopeners\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Opener, BZ2File, IndexedGzipFile\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecated\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecate_with_version\n",
      "File \u001b[0;32m/share/software/neuroImaging/fsl/6.0.6/lib/python3.10/site-packages/nibabel/casting.py:713\u001b[0m\n\u001b[1;32m    709\u001b[0m         floats\u001b[38;5;241m.\u001b[39mremove(np\u001b[38;5;241m.\u001b[39mlongdouble)\n\u001b[1;32m    710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(floats, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m f: type_info(f)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnmant\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 713\u001b[0m OK_FLOATS \u001b[38;5;241m=\u001b[39m \u001b[43mok_floats\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mable_int_type\u001b[39m(values):\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;124;03m\"\"\" Find the smallest integer numpy type to contain sequence `values`\u001b[39;00m\n\u001b[1;32m    718\u001b[0m \n\u001b[1;32m    719\u001b[0m \u001b[38;5;124;03m    Prefers uint to int if minimum is >= 0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;124;03m    True\u001b[39;00m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/share/software/neuroImaging/fsl/6.0.6/lib/python3.10/site-packages/nibabel/casting.py:707\u001b[0m, in \u001b[0;36mok_floats\u001b[0;34m()\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;124;03m\"\"\" Return floating point types sorted by precision\u001b[39;00m\n\u001b[1;32m    703\u001b[0m \n\u001b[1;32m    704\u001b[0m \u001b[38;5;124;03mRemove longdouble if it has no higher precision than float64\u001b[39;00m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;66;03m# copy float list so we don't change the numpy global\u001b[39;00m\n\u001b[0;32m--> 707\u001b[0m floats \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msctypes\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat\u001b[39m\u001b[38;5;124m'\u001b[39m][:]\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m best_float() \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mlongdouble \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39mlongdouble \u001b[38;5;129;01min\u001b[39;00m floats:\n\u001b[1;32m    709\u001b[0m     floats\u001b[38;5;241m.\u001b[39mremove(np\u001b[38;5;241m.\u001b[39mlongdouble)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/__init__.py:397\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m __expired_attributes__:\n\u001b[0;32m--> 397\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    398\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`np.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` was removed in the NumPy 2.0 release. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m__expired_attributes__[attr]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    400\u001b[0m     )\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchararray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    403\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`np.chararray` is deprecated and will be removed from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe main namespace in the future. Use an array with a string \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor bytes dtype instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: `np.sctypes` was removed in the NumPy 2.0 release. Access dtypes explicitly instead."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Path to your user-installed Python packages (adjust this path accordingly)\n",
    "user_site_packages = os.path.expanduser('~/.local/lib/python3.10/site-packages')\n",
    "\n",
    "# Add the directory to sys.path temporarily\n",
    "if user_site_packages not in sys.path:\n",
    "    sys.path.insert(0, user_site_packages)\n",
    "\n",
    "# Importing nilearn and itk after sys.path modification\n",
    "import nilearn\n",
    "from nilearn import image, plotting\n",
    "from nilearn.input_data import NiftiMasker\n",
    "\n",
    "import itk\n",
    "from distutils.version import StrictVersion as VS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ff01e42",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "`np.sctypes` was removed in the NumPy 2.0 release. Access dtypes explicitly instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Importing nilearn and itk after sys.path modification\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnilearn\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnilearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m image, plotting\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnilearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minput_data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NiftiMasker\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitk\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nilearn/image/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Mathematical operations working on Niimg-like objects.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mLike, for example, a (3+)D block of data, and an affine.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     binarize_img,\n\u001b[1;32m      8\u001b[0m     clean_img,\n\u001b[1;32m      9\u001b[0m     concat_imgs,\n\u001b[1;32m     10\u001b[0m     copy_img,\n\u001b[1;32m     11\u001b[0m     crop_img,\n\u001b[1;32m     12\u001b[0m     get_data,\n\u001b[1;32m     13\u001b[0m     high_variance_confounds,\n\u001b[1;32m     14\u001b[0m     index_img,\n\u001b[1;32m     15\u001b[0m     iter_img,\n\u001b[1;32m     16\u001b[0m     largest_connected_component_img,\n\u001b[1;32m     17\u001b[0m     load_img,\n\u001b[1;32m     18\u001b[0m     math_img,\n\u001b[1;32m     19\u001b[0m     mean_img,\n\u001b[1;32m     20\u001b[0m     new_img_like,\n\u001b[1;32m     21\u001b[0m     smooth_img,\n\u001b[1;32m     22\u001b[0m     swap_img_hemispheres,\n\u001b[1;32m     23\u001b[0m     threshold_img,\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     26\u001b[0m     coord_transform,\n\u001b[1;32m     27\u001b[0m     reorder_img,\n\u001b[1;32m     28\u001b[0m     resample_img,\n\u001b[1;32m     29\u001b[0m     resample_to_img,\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     32\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinarize_img\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_img\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mswap_img_hemispheres\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     54\u001b[0m ]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nilearn/image/image.py:14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnibabel\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Memory, Parallel, delayed\n",
      "File \u001b[0;32m/share/software/neuroImaging/fsl/6.0.6/lib/python3.10/site-packages/nibabel/__init__.py:40\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124mQuickstart\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124m==========\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124mFor more detailed information see the :ref:`manual`.\u001b[39m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# module imports\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m analyze \u001b[38;5;28;01mas\u001b[39;00m ana\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spm99analyze \u001b[38;5;28;01mas\u001b[39;00m spm99\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spm2analyze \u001b[38;5;28;01mas\u001b[39;00m spm2\n",
      "File \u001b[0;32m/share/software/neuroImaging/fsl/6.0.6/lib/python3.10/site-packages/nibabel/analyze.py:87\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\" Read / write access to the basic Mayo Analyze format\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m===========================\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03mconstrain the affine.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvolumeutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (native_code, swapped_code, make_dt_codes,\n\u001b[1;32m     88\u001b[0m                           shape_zoom_affine, array_from_file, seek_tell,\n\u001b[1;32m     89\u001b[0m                           apply_read_scaling)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marraywriters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (make_array_writer, get_slope_inter, WriterError,\n\u001b[1;32m     91\u001b[0m                            ArrayWriter)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrapstruct\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabeledWrapStruct\n",
      "File \u001b[0;32m/share/software/neuroImaging/fsl/6.0.6/lib/python3.10/site-packages/nibabel/volumeutils.py:21\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m reduce\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcasting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m shared_range, OK_FLOATS\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopeners\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Opener, BZ2File, IndexedGzipFile\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecated\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecate_with_version\n",
      "File \u001b[0;32m/share/software/neuroImaging/fsl/6.0.6/lib/python3.10/site-packages/nibabel/casting.py:713\u001b[0m\n\u001b[1;32m    709\u001b[0m         floats\u001b[38;5;241m.\u001b[39mremove(np\u001b[38;5;241m.\u001b[39mlongdouble)\n\u001b[1;32m    710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(floats, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m f: type_info(f)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnmant\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 713\u001b[0m OK_FLOATS \u001b[38;5;241m=\u001b[39m \u001b[43mok_floats\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mable_int_type\u001b[39m(values):\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;124;03m\"\"\" Find the smallest integer numpy type to contain sequence `values`\u001b[39;00m\n\u001b[1;32m    718\u001b[0m \n\u001b[1;32m    719\u001b[0m \u001b[38;5;124;03m    Prefers uint to int if minimum is >= 0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;124;03m    True\u001b[39;00m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/share/software/neuroImaging/fsl/6.0.6/lib/python3.10/site-packages/nibabel/casting.py:707\u001b[0m, in \u001b[0;36mok_floats\u001b[0;34m()\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;124;03m\"\"\" Return floating point types sorted by precision\u001b[39;00m\n\u001b[1;32m    703\u001b[0m \n\u001b[1;32m    704\u001b[0m \u001b[38;5;124;03mRemove longdouble if it has no higher precision than float64\u001b[39;00m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;66;03m# copy float list so we don't change the numpy global\u001b[39;00m\n\u001b[0;32m--> 707\u001b[0m floats \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msctypes\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat\u001b[39m\u001b[38;5;124m'\u001b[39m][:]\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m best_float() \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mlongdouble \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39mlongdouble \u001b[38;5;129;01min\u001b[39;00m floats:\n\u001b[1;32m    709\u001b[0m     floats\u001b[38;5;241m.\u001b[39mremove(np\u001b[38;5;241m.\u001b[39mlongdouble)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/__init__.py:397\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m __expired_attributes__:\n\u001b[0;32m--> 397\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    398\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`np.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` was removed in the NumPy 2.0 release. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m__expired_attributes__[attr]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    400\u001b[0m     )\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchararray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    403\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`np.chararray` is deprecated and will be removed from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe main namespace in the future. Use an array with a string \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor bytes dtype instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: `np.sctypes` was removed in the NumPy 2.0 release. Access dtypes explicitly instead."
     ]
    }
   ],
   "source": [
    "# Optionally remove from sys.path after use\n",
    "if user_site_packages in sys.path:\n",
    "    sys.path.remove(user_site_packages)\n",
    "\n",
    "# Checking ITK version requirement\n",
    "if VS(itk.Version.GetITKVersion()) < VS(\"5.0.0\"):\n",
    "    print(\"ITK 5.0.0 or newer is required.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Proceed with the rest of your imports and configuration\n",
    "import pip\n",
    "from time import perf_counter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from sklearn import cluster\n",
    "\n",
    "from skimage import exposure\n",
    "from skimage.draw import line_nd, line_aa\n",
    "from skimage.transform import rescale, resize\n",
    "from skimage.filters import gaussian\n",
    "from skimage import morphology\n",
    "from skimage.filters import threshold_otsu, threshold_local\n",
    "from skimage.morphology import binary_dilation\n",
    "\n",
    "from scipy import ndimage as ndi\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "from numba import njit, jit, prange\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = [15, 15]\n",
    "mpl.rcParams.update({'font.size': 22})\n",
    "np.set_printoptions(formatter={'float_kind':'{:0.4f}'.format})\n",
    "\n",
    "import os  # Import the os module at the beginning of your script\n",
    "\n",
    "\n",
    "###############################################\n",
    "### Set these:\n",
    "###############################################\n",
    "root = \"/exports/gorter-hpc/users/ninafultz/highres_pvs/20240307_PVS3\"  # Root folder of your project\n",
    "out_folder = os.path.join(root, \"segment/\")\n",
    "biasfield_folder = os.path.join(root, \"biasfield/\")\n",
    "\n",
    "# Change current working directory to biasfield_folder\n",
    "os.chdir(biasfield_folder)\n",
    "\n",
    "input_img_url = os.path.join(biasfield_folder, \"merged_last_three_DelRec_-_pvs_70slices_ME.nii.gz\")\n",
    "input_mask_url = os.path.join(biasfield_folder, \"combined_mask.nii.gz\")\n",
    "output_prefix = os.path.join(out_folder, \"qsm_vasc_\")\n",
    "out_hessian = output_prefix + \"_hessian.nii.gz\"\n",
    "\n",
    "in_image = itk.imread(input_img_url, itk.F)\n",
    "\n",
    "if not os.path.exists(out_folder):\n",
    "    os.makedirs(out_folder) \n",
    "\n",
    "try:\n",
    "    # Attempt to read the image using itk.imread\n",
    "    in_image = itk.imread(input_img_url, itk.F)\n",
    "\n",
    "    # Get voxel size and size of the image\n",
    "    voxel_size = in_image.GetSpacing()\n",
    "    size = in_image.GetLargestPossibleRegion().GetSize()\n",
    "\n",
    "    # Example: Print out voxel size and image size\n",
    "    print(f\"Voxel Size: {voxel_size}\")\n",
    "    print(f\"Image Size: {size}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error reading image: {e}\")\n",
    "    \n",
    "    \n",
    "in_image   = itk.imread(input_img_url, itk.F)               # don't touch this\n",
    "voxel_size = in_image.GetSpacing()                          # don't touch this\n",
    "size       = in_image.GetLargestPossibleRegion().GetSize()  # don't touch this\n",
    "\n",
    "\n",
    "brightvessels = False                  # Bright=ToF, Dark=SWI,T2star\n",
    "sigma_minimum = np.mean(voxel_size)    # min voxel size (or mean), can be a little higher\n",
    "sigma_maximum = sigma_minimum*1.95     # a bit less than twice\n",
    "number_of_sigma_steps=15               #The more the better, between 10 and 15 is good\n",
    "fact_version = True                    #Factory analysis (new) vs Max pooling (classic)\n",
    "\n",
    "alpha=0.8                     #In theory, no need to adjust this\n",
    "beta=1.0                      #In theory, no need to adjust this\n",
    "gamma=100                     #50 #Play with this to ajust for the noise/blur in the vascular segmentation\n",
    "scaleoutput = True            #Do. Not. Touch. That.\n",
    "\n",
    "otsu_offset=0.3              #Between -1.0 and 1.0 -> higher = more vessels when thresholded\n",
    "    \n",
    "MIP_half_thickness=8           # Half-thickness of the MIP view\n",
    "window_half_size=50            # 30 = 60x60 zoomed window\n",
    "mip_dir=\"ax\" #\"sag\"  #\"cor\"    # choice between \"sag\" \"cor\" or other (default)\n",
    "\n",
    "# Center of the window\n",
    "ix, iy, iz  = np.int(size[0]/1.6), np.int(size[1]/1.5), np.int(size[2]/2.)\n",
    "\n",
    "\n",
    "# Load images and print them\n",
    "mask_img   = image.load_img(input_mask_url)\n",
    "mask       = mask_img.get_data()\n",
    "input_img  = image.load_img(input_img_url)\n",
    "voxel_size = mask_img.header.get_zooms()[0:2]\n",
    "\n",
    "print(\"Voxel size is: \", voxel_size)\n",
    "print(\"Image dim is: \", mask_img.shape)\n",
    "#print(\"Image dim is: \", size)\n",
    "\n",
    "if mip_dir == 'sag':\n",
    "    sx, sy, sz  = MIP_half_thickness, window_half_size, window_half_size\n",
    "    mip_axis = 0\n",
    "\n",
    "    dx, ux = np.int(ix-sx), np.int(ix+sx)\n",
    "    dy, uy = np.int(iy-sy), np.int(iy+sy)  \n",
    "    dz, uz = np.int(iz-sz), np.int(iz+sz) \n",
    "\n",
    "    mip_cube = tuple([ slice(dx,ux,1), slice(dy,uy,1),   slice(dz,uz,1)    ])\n",
    "    mip_view = tuple([ slice(dx,ux,1), slice(None,None), slice(None,None)  ])\n",
    "    sng_cube = tuple([ ix,             slice(dy,uy,1),   slice(dz,uz,1)    ])\n",
    "    sng_view = tuple([ ix,             slice(None,None), slice(None,None)  ])\n",
    "elif mip_dir == 'cor':\n",
    "    sx, sy, sz  = window_half_size, MIP_half_thickness, window_half_size\n",
    "    mip_axis = 1\n",
    "\n",
    "    dx, ux = np.int(ix-sx), np.int(ix+sx)\n",
    "    dy, uy = np.int(iy-sy), np.int(iy+sy)  \n",
    "    dz, uz = np.int(iz-sz), np.int(iz+sz) \n",
    "\n",
    "    mip_cube = tuple([ slice(dx,ux,1),   slice(dy,uy,1), slice(dz,uz,1)    ])\n",
    "    mip_view = tuple([ slice(None,None), slice(dy,uy,1), slice(None,None)  ])\n",
    "    sng_cube = tuple([ slice(dx,ux,1),   iy,             slice(dz,uz,1)    ])\n",
    "    sng_view = tuple([ slice(None,None), iy,             slice(None,None)  ])\n",
    "\n",
    "else :\n",
    "    sx, sy, sz  = window_half_size, window_half_size, MIP_half_thickness\n",
    "    mip_axis = 2\n",
    "\n",
    "    dx, ux = np.int(ix-sx), np.int(ix+sx)\n",
    "    dy, uy = np.int(iy-sy), np.int(iy+sy)  \n",
    "    dz, uz = np.int(iz-sz), np.int(iz+sz)  \n",
    "\n",
    "    mip_cube = tuple([ slice(dx,ux,1),    slice(dy,uy,1),    slice(dz,uz,1) ])\n",
    "    mip_view = tuple([ slice(None, None), slice(None, None), slice(dz,uz,1) ])\n",
    "    sng_cube = tuple([ slice(dx,ux,1),    slice(dy,uy,1),    iz             ])\n",
    "    sng_view = tuple([ slice(None, None), slice(None, None), iz             ])\n",
    "    \n",
    "#Set-up window variables\n",
    "print(\"Center: \",      ix, iy, iz)\n",
    "print(\"Zoom window: \", sx, sy, sz)\n",
    "print(\"Low limit: \",   dx, dy, dz)\n",
    "print(\"High limit: \",  ux, uy, uz)\n",
    "print(\"Size of single will be: \",        mask[sng_view].T.shape)\n",
    "print(\"Size of zoomed single will be: \", mask[sng_cube].T.shape)\n",
    "print(\"Size of MIP will be: \",           np.min(mask[mip_view], axis=mip_axis).shape)\n",
    "print(\"Size of zoomed MIP will be: \",    np.min(mask[mip_cube], axis=mip_axis).shape)\n",
    "   \n",
    "#Plot the loaded images\n",
    "fig, axes = plt.subplots(1, 3, figsize=(28, 20))\n",
    "fig.tight_layout()\n",
    "ax = axes.flatten()\n",
    "ax[0].imshow(input_img.get_data()[sng_view].T, origin='lower', cmap=plt.cm.gray)\n",
    "ax[1].imshow(np.min(input_img.get_data()[mip_view], axis=mip_axis).T, origin='lower', cmap=plt.cm.gray, alpha=1.0)\n",
    "ax[2].imshow(np.min(input_img.get_data()[mip_cube], axis=mip_axis).T, origin='lower', cmap=plt.cm.gray, alpha=1.0)\n",
    "plt.title(\"Data loaded\")\n",
    "plt.savefig(\"Loaded_data_fig.png\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def FA_combine_scales(img_4D, mask, sigma_min, sigma_max):\n",
    "    masker = NiftiMasker()\n",
    "    data_masked = masker.fit_transform(img_4D)\n",
    "    print(np.max(data_masked))\n",
    "    assert np.max(data_masked) > 0, \"Mask and data are not in same space/orient\"\n",
    "\n",
    "    \n",
    "    p0, p99 = np.percentile(data_masked, (0, 99.8)) \n",
    "    data_masked = exposure.rescale_intensity(data_masked, in_range=(p0,p99))\n",
    "\n",
    "    #print(data_masked.shape)\n",
    "\n",
    "    print(np.max(data_masked))\n",
    "\n",
    "    noise_variance_init=np.geomspace(sigma_min, sigma_max, data_masked.shape[0]) - sigma_min #[0:16]\n",
    "    print(\"Noise variance: \", noise_variance_init)\n",
    "\n",
    "    method = FactorAnalysis(n_components=1, \n",
    "                            noise_variance_init=noise_variance_init,\n",
    "                            #noise_variance_init=np.geomspace(sigma_minimum, sigma_maximum, number_of_sigma_steps),\n",
    "                            tol=0.005, max_iter=200)\n",
    "    method.fit(data_masked.T)\n",
    "    X_reduced = method.transform(data_masked.T)\n",
    "    X_reduced[X_reduced<0] = 0\n",
    "    #print(X_reduced.shape)\n",
    "\n",
    "    #print(X_reduced.noise_variance_array_)\n",
    "    fact_img = masker.inverse_transform(X_reduced.T)\n",
    "    fact_img = image.index_img(fact_img,0)\n",
    "    \n",
    "    factimg=nilearn.image.math_img(\"np.where(x<0, 0, x)\", x=fact_img)\n",
    "    \n",
    "    p0, p99 = np.percentile(factimg.get_data(), (0, 99.8)) \n",
    "    factdata = exposure.rescale_intensity(factimg.get_data(), in_range=(p0,p99))\n",
    "\n",
    "    return factdata\n",
    "\n",
    "def skeletonize_mask(i_thresh_img, i_voxel_size=0.6, i_factor=2.0):\n",
    "    thresh_data = i_thresh_img.get_data()\n",
    "    dil_thresh = binary_dilation(thresh_data)\n",
    "    skel = morphology.skeletonize_3d(dil_thresh)\n",
    "    #skel = image.load_img(outfold + output_prefix + \"_VED_skel.nii.gz\").get_data()\n",
    "    skel[skel > 0] = 1.0\n",
    "\n",
    "    prox_sampling = int(i_voxel_size/i_factor)\n",
    "\n",
    "    #up_thresh = rescale(thresh_data, i_factor)\n",
    "    #up_distance = ndi.distance_transform_edt(up_thresh, sampling=prox_sampling)\n",
    "    #distance = resize(up_distance, skel.shape) * thresh_data\n",
    "    distance = ndi.distance_transform_edt(thresh_data, sampling=i_voxel_size)\n",
    "    centerdia = skel * distance\n",
    "\n",
    "    #up_i_thresh = 1.0 - up_thresh\n",
    "    up_i_thresh = 1.0 - thresh_data\n",
    "    #up_i_distance = ndi.distance_transform_edt(up_i_thresh, sampling=prox_sampling) #get from header instead\n",
    "    up_i_distance = ndi.distance_transform_edt(up_i_thresh, sampling=i_voxel_size) #get from header instead\n",
    "    inverted_distance = up_i_distance  * up_i_thresh\n",
    "\n",
    "    return skel, centerdia, distance, inverted_distance\n",
    "\n",
    "def otsu_3D(i_img, offset=0.2):\n",
    "\n",
    "    p0, p99 = np.percentile(i_img.get_data(), (0, 99.8)) \n",
    "    datacopy = exposure.rescale_intensity(i_img.get_data(), in_range=(p0,p99))\n",
    "\n",
    "    #datacopy = fact_img.get_data()\n",
    "    threshold_x = np.zeros_like(datacopy)\n",
    "    threshold_y = np.zeros_like(datacopy)\n",
    "    threshold_z = np.zeros_like(datacopy)\n",
    "\n",
    "    offset = 0.2\n",
    "    #threshold_local\n",
    "    for x in range(datacopy.shape[0]):\n",
    "        if np.max(datacopy[x, :, :]) == 0.0:\n",
    "            val = 1.0\n",
    "        else:\n",
    "            img_adapteq = exposure.equalize_adapthist(datacopy[x,:,:], clip_limit=0.01)\n",
    "            val = threshold_otsu(img_adapteq, nbins=400 )\n",
    "            val = max(0.0, val-offset)\n",
    "            threshold_x[x, :, :] = val\n",
    "\n",
    "    for y in range(datacopy.shape[1]):\n",
    "        if np.max(datacopy[:, y, :]) == 0.0:\n",
    "            val = 1.0\n",
    "        else:\n",
    "            img_adapteq = exposure.equalize_adapthist(datacopy[:,y,:], clip_limit=0.01)\n",
    "            val = threshold_otsu(img_adapteq, nbins=400 )\n",
    "            val = max(0.0, val-offset)\n",
    "            threshold_y[:, y, :] = val\n",
    "\n",
    "    for z in range(datacopy.shape[2]):\n",
    "        if np.max(datacopy[:, :, z]) == 0.0:\n",
    "            val = 1.0\n",
    "        else:\n",
    "            img_adapteq = exposure.equalize_adapthist(datacopy[:,:,z], clip_limit=0.01)\n",
    "            val = threshold_otsu(img_adapteq, nbins=400 )\n",
    "            val = max(0.0, val-offset)\n",
    "            threshold_z[:, :, z] = val\n",
    "\n",
    "    threshold = np.stack((threshold_x, threshold_y, threshold_z), axis=3)\n",
    "    threshold_min = np.min(threshold, axis=3)\n",
    "\n",
    "    return datacopy > threshold_min\n",
    "\n",
    "print(\"+-+-+- DOING MULTI-SCALE METHOD\")\n",
    "\n",
    "ImageType = type(in_image)\n",
    "Dimension = in_image.GetImageDimension()  #would be 3\n",
    "\n",
    "HessianPixelType = itk.SymmetricSecondRankTensor[itk.D, Dimension]\n",
    "HessianImageType = itk.Image[HessianPixelType, Dimension]\n",
    "print(HessianImageType)\n",
    "\n",
    "objectness_filter = itk.HessianToObjectnessMeasureImageFilter[HessianImageType, ImageType].New()\n",
    "objectness_filter.SetBrightObject(brightvessels)\n",
    "objectness_filter.SetScaleObjectnessMeasure(scaleoutput)  #WAS FALSE per default\n",
    "objectness_filter.SetAlpha(alpha) # 0.5 default\n",
    "objectness_filter.SetBeta(beta)   # should be 1.0\n",
    "objectness_filter.SetGamma(gamma) # was 5.0 in example, 300 at high gamma.. higher = less noise\n",
    "\n",
    "\n",
    "outfold = output_prefix + '_scales/'\n",
    "if not os.path.exists(outfold):\n",
    "    os.makedirs(outfold)\n",
    "\n",
    "\n",
    "print(\"sigma_minimum is: \" + str(sigma_minimum))\n",
    "print(\"sigma_maximum is: \" + str(sigma_maximum))\n",
    "for step, sigma in enumerate(np.geomspace(sigma_minimum, sigma_maximum, number_of_sigma_steps)):\n",
    "\n",
    "    scale_str = str(int(sigma*100000)).zfill(8)\n",
    "    \n",
    "    out_scale_prefix=outfold + output_prefix + \"_\" + scale_str\n",
    "\n",
    "    if not os.path.exists(out_scale_prefix + \"_VED.nii.gz\"):\n",
    "        print(\" +-+- DOING sigma scale \" + str(sigma) + \": \" + str(step+1) + \" of \" + str(number_of_sigma_steps))\n",
    "        multi_scale_filter = itk.MultiScaleHessianBasedMeasureImageFilter[ImageType, HessianImageType, ImageType].New()\n",
    "        multi_scale_filter.SetInput(in_image)\n",
    "        multi_scale_filter.SetHessianToMeasureFilter(objectness_filter)\n",
    "        multi_scale_filter.SetSigmaStepMethodToLogarithmic()\n",
    "        multi_scale_filter.SetSigmaMinimum(sigma)\n",
    "        multi_scale_filter.SetSigmaMaximum(sigma)\n",
    "        multi_scale_filter.SetNumberOfSigmaSteps(1)\n",
    "        multi_scale_filter.SetGenerateHessianOutput(True)\n",
    "        multi_scale_filter.Update()\n",
    "\n",
    "        print(\"  +- ..write VED of sigma \" + str(sigma) + \": \" + str(step+1) + \" of \" + str(number_of_sigma_steps))\n",
    "        Hessian_output=multi_scale_filter.GetHessianOutput()\n",
    "        VED_output=multi_scale_filter.GetOutput()\n",
    "        size=Hessian_output.GetBufferedRegion().GetSize()\n",
    "\n",
    "        hessian_view = itk.GetArrayViewFromImage(Hessian_output)\n",
    "        hessian_array = np.transpose(hessian_view, (2,1,0,3))\n",
    "        ved_view = itk.GetArrayViewFromImage(VED_output)\n",
    "        ved_array = np.transpose(ved_view, (2,1,0))\n",
    "        ved_array = ved_array*mask\n",
    "\n",
    "        #print(np.min(ved_array))\n",
    "        #print(np.max(ved_array))\n",
    "        p0, p99 = np.percentile(ved_array, (0, 99.8)) \n",
    "        ved_array = exposure.rescale_intensity(ved_array, in_range=(p0,p99))\n",
    "        #print(np.min(ved_array))\n",
    "        #print(np.max(ved_array))\n",
    "    \n",
    "\n",
    "        ved_img = nilearn.image.new_img_like(input_img, ved_array, copy_header=True)\n",
    "        ved_img.to_filename(out_scale_prefix + \"_VED.nii.gz\")\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(28, 14))\n",
    "        fig.tight_layout()\n",
    "        ax = axes.flatten()\n",
    "        ax[0].imshow(np.min(input_img.get_data()[mip_view], axis=mip_axis).T, origin='lower')\n",
    "        ax[1].imshow(np.min(input_img.get_data()[mip_view], axis=mip_axis).T, origin='lower', cmap=plt.cm.gray, alpha=1.0)\n",
    "        ax[1].imshow(np.max(     ved_array[mip_view], axis=mip_axis).T, origin='lower', cmap='viridis', alpha=0.7)\n",
    "        ax[2].imshow(np.max(     ved_array[mip_view], axis=mip_axis).T, origin='lower')\n",
    "        plt.title(\"VED for: \" + str(sigma) + \": \" + str(step+1) + \" of \" + str(number_of_sigma_steps))\n",
    "        plt.savefig(out_scale_prefix + '_fig.png', bbox_inches='tight')\n",
    "        plt.show()       \n",
    "        \n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=(28, 14))\n",
    "        fig.tight_layout()\n",
    "        ax = axes.flatten()\n",
    "        ax[0].imshow(np.min(input_img.get_data()[mip_cube], axis=mip_axis).T, origin='lower')\n",
    "        ax[1].imshow(np.min(input_img.get_data()[mip_cube], axis=mip_axis).T, origin='lower', cmap=plt.cm.gray, alpha=1.0)\n",
    "        ax[1].imshow(np.max(     ved_array[mip_cube], axis=mip_axis).T, origin='lower', cmap='viridis', alpha=0.7)\n",
    "        ax[2].imshow(np.max(     ved_array[mip_cube], axis=mip_axis).T, origin='lower')\n",
    "        plt.title(\"VED for: \" + str(sigma) + \": \" + str(step+1) + \" of \" + str(number_of_sigma_steps))\n",
    "        plt.savefig(out_scale_prefix + '_ZOOM.png', bbox_inches='tight')\n",
    "        plt.show()\n",
    "        #plotting.plot_epi(ved_img, input_img, crop=True, filename=out_scale_prefix + \"_VED.png\", dpi=150)\n",
    "\n",
    "        \n",
    "\n",
    "out_classic_prefix=output_prefix + \"_CLASSIC\"\n",
    "\n",
    "if not os.path.exists(out_classic_prefix + \"_VED.nii.gz\"):\n",
    "   \n",
    "    print(\"+-+-+- DOING CLASSIC METHOD\")\n",
    "    multi_scale_filter = itk.MultiScaleHessianBasedMeasureImageFilter[ImageType, HessianImageType, ImageType].New()\n",
    "    multi_scale_filter.SetInput(in_image)\n",
    "    multi_scale_filter.SetHessianToMeasureFilter(objectness_filter)\n",
    "    multi_scale_filter.SetSigmaStepMethodToLogarithmic()\n",
    "    multi_scale_filter.SetSigmaMinimum(sigma_minimum)\n",
    "    multi_scale_filter.SetSigmaMaximum(sigma_maximum)\n",
    "    multi_scale_filter.SetNumberOfSigmaSteps(number_of_sigma_steps)\n",
    "    multi_scale_filter.SetGenerateHessianOutput(True)\n",
    "    multi_scale_filter.Update()\n",
    "    print(\"+-+-+- DONE CLASSIC\")\n",
    "\n",
    "    print(\"+-+- Check output size\")\n",
    "    Hessian_output=multi_scale_filter.GetHessianOutput()\n",
    "    VED_output=multi_scale_filter.GetOutput()\n",
    "    size=Hessian_output.GetBufferedRegion().GetSize()\n",
    "    print(\"%d,%d,%d\"%(size[0],size[1],size[2]))\n",
    "\n",
    "\n",
    "    print(\"+-+- Making new image\")\n",
    "    hessian_view = itk.GetArrayViewFromImage(Hessian_output)\n",
    "    hessian_array = np.transpose(hessian_view, (2,1,0,3))\n",
    "    ved_view = itk.GetArrayViewFromImage(VED_output)\n",
    "    ved_array = np.transpose(ved_view, (2,1,0))\n",
    "    print(hessian_array.shape)\n",
    "\n",
    "    ved_array = np.transpose(ved_view, (2,1,0))\n",
    "    ved_array = ved_array*mask\n",
    "\n",
    "    print(np.min(ved_array))\n",
    "    print(np.max(ved_array))\n",
    "    p0, p99 = np.percentile(ved_array, (0, 99.8)) \n",
    "    ved_array = exposure.rescale_intensity(ved_array, in_range=(p0,p99))\n",
    "    print(np.min(ved_array))\n",
    "    print(np.max(ved_array))\n",
    "\n",
    "    ved_img = nilearn.image.new_img_like(input_img, ved_array, copy_header=True)\n",
    "    ved_img.to_filename(out_classic_prefix + \"_VED.nii.gz\")\n",
    "\n",
    "    ####\n",
    "    # Plot: MIP around shape[1]/2\n",
    "    ###\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(28, 14))\n",
    "    fig.tight_layout()\n",
    "    ax = axes.flatten()\n",
    "    ax[0].imshow(np.min(input_img.get_data()[mip_view], axis=mip_axis).T, origin='lower')\n",
    "    ax[1].imshow(np.min(input_img.get_data()[mip_view], axis=mip_axis).T, origin='lower', cmap=plt.cm.gray, alpha=1.0)\n",
    "    ax[1].imshow(np.max(     ved_array[mip_view], axis=mip_axis).T, origin='lower', cmap='viridis', alpha=0.7)\n",
    "    ax[2].imshow(np.max(     ved_array[mip_view], axis=mip_axis).T, origin='lower')\n",
    "    plt.title(\"VED for: \" + str(sigma) + \": \" + str(step+1) + \" of \" + str(number_of_sigma_steps))\n",
    "    plt.savefig(out_classic_prefix + '_fig.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "            \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(28, 14))\n",
    "    fig.tight_layout()\n",
    "    ax = axes.flatten()\n",
    "    ax[0].imshow(np.min(input_img.get_data()[mip_cube], axis=mip_axis).T, origin='lower')\n",
    "    ax[1].imshow(np.min(input_img.get_data()[mip_cube], axis=mip_axis).T, origin='lower', cmap=plt.cm.gray, alpha=1.0)\n",
    "    ax[1].imshow(np.max(     ved_array[mip_cube], axis=mip_axis).T, origin='lower', cmap='viridis', alpha=0.7)\n",
    "    ax[2].imshow(np.max(     ved_array[mip_cube], axis=mip_axis).T, origin='lower')\n",
    "    plt.title(\"VED for: \" + str(sigma) + \": \" + str(step+1) + \" of \" + str(number_of_sigma_steps))\n",
    "    plt.savefig(out_classic_prefix + '_ZOOM.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    hessian_img = nilearn.image.new_img_like(input_img, hessian_array, copy_header=True)\n",
    "    hessian_img.to_filename(out_classic_prefix + \"_best_hessian.nii.gz\")\n",
    "\n",
    "    ved_img = nilearn.image.new_img_like(input_img, ved_array, copy_header=True)\n",
    "    ved_img.to_filename(out_classic_prefix + \"_VED.nii.gz\")\n",
    "\n",
    "else:\n",
    "    ved_img =       nilearn.image.load_img(out_classic_prefix + \"_VED.nii.gz\")\n",
    "    hessian_img =   nilearn.image.load_img(out_classic_prefix + \"_best_hessian.nii.gz\")\n",
    "    ved_array =     ved_img.get_data()\n",
    "    hessian_array = hessian_img.get_data()\n",
    "    size = ved_array.shape\n",
    "\n",
    "print(\"+-+-+- Load VED images\")\n",
    "VED_images=nilearn.image.load_img(outfold + output_prefix + \"*_VED.nii.gz\", wildcards=True)\n",
    "print(\"Shape is: \", VED_images.shape)\n",
    "\n",
    "if not os.path.exists(outfold + output_prefix + \"_VED_max.nii.gz\"):\n",
    "    print(\"+-+- Compute max image\")\n",
    "    max_VED_images=nilearn.image.math_img(\"np.max(a, axis=3)\", a=VED_images)\n",
    "    max_VED_images.to_filename(outfold + output_prefix + \"_VED_max.nii.gz\")\n",
    "else:\n",
    "    max_VED_images = image.load_img(outfold + output_prefix + \"_VED_max.nii.gz\")\n",
    "\n",
    "if not os.path.exists(outfold + output_prefix + \"_VED_FactorAnalysis.nii.gz\"):\n",
    "    print(\"+-+- Compute factory analysis\")\n",
    "    fact_data = FA_combine_scales(VED_images, mask_img, sigma_minimum, sigma_maximum)\n",
    "    fact_img = image.new_img_like(input_img, fact_data)\n",
    "    fact_img.to_filename(outfold + output_prefix + \"_VED_FactorAnalysis.nii.gz\") \n",
    "else:\n",
    "    fact_img=image.load_img(outfold + output_prefix + \"_VED_FactorAnalysis.nii.gz\")\n",
    "    \n",
    "if fact_version:\n",
    "    ved_img = fact_img\n",
    "else:\n",
    "    ved_img = max_VED_images\n",
    "\n",
    "if not os.path.exists(outfold + output_prefix + \"_VED_otsu_thresh.nii.gz\"):\n",
    "    print(\"+-+- Compute OTSU 3D analysis\")\n",
    "    thr_data = otsu_3D(ved_img, offset=otsu_offset)\n",
    "    otsu_tresh_img = image.new_img_like(ved_img, thr_data)\n",
    "    otsu_tresh_img.to_filename(outfold + output_prefix + \"_VED_otsu_thresh.nii.gz\")\n",
    "else:\n",
    "    otsu_tresh_img=image.load_img(outfold + output_prefix + \"_VED_otsu_thresh.nii.gz\")\n",
    "\n",
    "    \n",
    "thresh_img = otsu_tresh_img\n",
    "ved_data = fact_img.get_data()\n",
    "thresh_data = otsu_tresh_img.get_data()\n",
    "\n",
    "#Print results\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(28, 12))\n",
    "fig.tight_layout()\n",
    "ax = axes.flatten()\n",
    "ax[0].imshow(input_img.get_data()[sng_view].T, origin='lower')\n",
    "ax[1].imshow(ved_img.get_data()[sng_view].T, origin='lower', cmap='viridis')\n",
    "ax[2].imshow(otsu_tresh_img.get_data()[sng_view].T, origin='lower', cmap='viridis')\n",
    "plt.savefig(outfold + output_prefix + '_VED_processed.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(28, 12))\n",
    "fig.tight_layout()\n",
    "ax = axes.flatten()\n",
    "ax[0].imshow(np.min(input_img.get_data()[mip_view], axis=mip_axis).T, origin='lower')\n",
    "ax[1].imshow(np.max(ved_img.get_data()[mip_view], axis=mip_axis).T, origin='lower', cmap='viridis')\n",
    "ax[2].imshow(np.max(otsu_tresh_img.get_data()[mip_view], axis=mip_axis).T, origin='lower', cmap='viridis')\n",
    "plt.savefig(outfold + output_prefix + '_VED_processed_MIP.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(28, 12))\n",
    "fig.tight_layout()\n",
    "ax = axes.flatten()\n",
    "ax[0].imshow(np.min(input_img.get_data()[mip_cube], axis=mip_axis).T, origin='lower')\n",
    "ax[1].imshow(np.max(ved_img.get_data()[mip_cube], axis=mip_axis).T, origin='lower', cmap='viridis')\n",
    "ax[2].imshow(np.max(otsu_tresh_img.get_data()[mip_cube], axis=mip_axis).T, origin='lower', cmap='viridis')\n",
    "plt.savefig(outfold + output_prefix + '_VED_processed_MIP_ZOOM.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#skeletonize and distance maps\n",
    "if not os.path.exists(outfold + output_prefix + '_VED_inverted_distance.nii.gz') :  \n",
    "    print(\"+-+-+- skeletonize and compute distance\")        \n",
    "    thresh_img = otsu_tresh_img\n",
    "    skel, centerdia, distance, inverted_distance = \\\n",
    "        skeletonize_mask(thresh_img, i_voxel_size=np.min(voxel_size), i_factor=3.0)\n",
    "\n",
    "    img = image.new_img_like(thresh_img, skel)\n",
    "    img.to_filename(outfold + output_prefix + \"_VED_skel.nii.gz\")\n",
    "    img = image.new_img_like(thresh_img, centerdia)\n",
    "    img.to_filename(outfold + output_prefix + \"_VED_centerdia.nii.gz\")\n",
    "    img = image.new_img_like(thresh_img, distance)\n",
    "    img.to_filename(outfold + output_prefix + \"_VED_distance.nii.gz\")\n",
    "    img = image.new_img_like(thresh_img, inverted_distance)\n",
    "    img.to_filename(outfold + output_prefix + \"_VED_inverted_distance.nii.gz\")\n",
    "else :\n",
    "    skel =              image.load_img(outfold + output_prefix + \"_VED_skel.nii.gz\").get_data()\n",
    "    centerdia =         image.load_img(outfold + output_prefix + \"_VED_centerdia.nii.gz\").get_data()\n",
    "    distance =          image.load_img(outfold + output_prefix + \"_VED_distance.nii.gz\").get_data()\n",
    "    inverted_distance = image.load_img(outfold + output_prefix + \"_VED_inverted_distance.nii.gz\").get_data()\n",
    "     \n",
    "\n",
    "#print results\n",
    "fig, axes = plt.subplots(1, 4, figsize=(25, 12))\n",
    "ax = axes.flatten()\n",
    "ax[0].imshow(np.max(ved_img.get_data()[mip_view], axis=mip_axis).T, origin='lower', cmap=\"gray\")\n",
    "ax[0].imshow(np.max(skel[mip_view], axis=mip_axis).T, origin='lower', cmap='viridis', alpha=0.5)\n",
    "ax[1].imshow(np.max(centerdia[mip_view], axis=mip_axis).T, origin='lower', cmap=\"viridis\")\n",
    "ax[2].imshow(np.mean(distance[mip_view], axis=mip_axis).T, origin='lower', cmap=\"viridis\")\n",
    "ax[3].imshow(np.max(skel[mip_view], axis=mip_axis).T, origin='lower', cmap=\"gray\")\n",
    "ax[3].imshow(np.mean(inverted_distance[mip_view], axis=mip_axis).T, origin='lower', cmap=\"viridis\", alpha=0.85)\n",
    "fig.savefig(outfold + output_prefix + '_skeletonized.png', bbox_inches='tight')\n",
    "fig.show()\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(25, 12))\n",
    "ax = axes.flatten()\n",
    "ax[0].imshow(np.max(ved_img.get_data()[mip_cube], axis=mip_axis).T, origin='lower', cmap=\"gray\")\n",
    "ax[0].imshow(np.max(skel[mip_cube], axis=mip_axis).T, origin='lower', cmap='viridis', alpha=0.5)\n",
    "ax[1].imshow(np.max(centerdia[mip_cube], axis=mip_axis).T, origin='lower', cmap=\"gray\")\n",
    "ax[2].imshow(np.mean(distance[mip_cube], axis=mip_axis).T, origin='lower', cmap=\"viridis\")\n",
    "ax[3].imshow(np.max(skel[mip_cube], axis=mip_axis).T, origin='lower', cmap=\"gray\")\n",
    "ax[3].imshow(np.mean(inverted_distance[mip_cube], axis=mip_axis).T, origin='lower', cmap=\"viridis\", alpha=0.85)\n",
    "fig.savefig(outfold + output_prefix + '_skeletonized_zoom.png', bbox_inches='tight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2243f326",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
